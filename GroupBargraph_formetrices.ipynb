{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f8d45-3387-4358-831c-3ccbd18733f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the metric names and sample values (replace with your actual values)\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'RÂ² Score']\n",
    "model_scores = {\n",
    "    'SVR':     [1.56, 4.84, 2.20, 0.81],\n",
    "    'RFR':     [1.73, 5.13, 2.26, 0.65],\n",
    "    'GBR':     [1.96, 6.04, 2.46, 0.59],\n",
    "    'XGBoost': [1.84, 5.80, 2.40, 0.60],\n",
    "    'DT':      [2.58, 10.88, 3.30, 0.26],\n",
    "    'LightGBM':[1.80, 5.40, 2.32, 0.63],\n",
    "    'KNN':     [2.00, 7.68, 2.77, 0.48],\n",
    "}\n",
    "\n",
    "models = list(model_scores.keys())\n",
    "n_metrics = len(metrics)\n",
    "x = np.arange(n_metrics)\n",
    "width = 0.1\n",
    "\n",
    "# Plot grouped bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, model in enumerate(models):\n",
    "    values = model_scores[model]\n",
    "    offset = (i - len(models) / 2) * width + width / 2\n",
    "    plt.bar(x + offset, values, width=width, label=model)\n",
    "\n",
    "# Add labels and formatting\n",
    "plt.xticks(x, metrics)\n",
    "plt.xlabel('Evaluation Metrics')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of ML Models Across Evaluation Metrics')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
